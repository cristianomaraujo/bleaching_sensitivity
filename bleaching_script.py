# -*- coding: utf-8 -*-
"""Bleaching - VERSÃO FINAL COM IC95%

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b6nloF2AiZMw45IIZyiLGdgo2z11NULj

# **Library and packages**
"""

import pandas as pd
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import numpy as np
from sklearn import metrics
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import roc_curve, auc

"""# **Dataset**"""

file_path = '/content/dataset_risk.xlsx'
df = pd.read_excel(file_path)

df.columns

"""# **Data preprocessing**"""

colunas = ['Age ',  'Vita Classical (Baseline color)','Concentration',
       'pH', 'Risk','Number of sessions']

df = df.loc[:, colunas]

coluna_grupo = df.pop('Risk')
df.insert(0, 'Risk', coluna_grupo)

df.info()
df.columns

"""# **Classification Model building**

**Data splitting**
"""

X = df.iloc[:,1:]
y = df.iloc[:,0]

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

#Dataset split
RANDOM_STATE = 88
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20,random_state=RANDOM_STATE, shuffle=True)


##Data normalization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#SMOTE - oversampling (only train data)
oversample = SMOTE(random_state=RANDOM_STATE)
X_train, y_train = oversample.fit_resample(X_train, y_train)


print(f"Train data shape of X = {X_train.shape} and Y = {y_train.shape}")
print(f"Test data shape of X = {X_test.shape} and Y = {y_test.shape}")

# K-Folds
fold = 5

"""**GRADIENT BOOSTING**"""

import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV, KFold, cross_val_predict, train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, recall_score, f1_score
from sklearn.utils import resample

param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

gb_model = GradientBoostingClassifier(random_state=RANDOM_STATE)
grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_gb_model = grid_search.best_estimator_
best_gb_model.fit(X_train, y_train)
y_pred_test_gb = best_gb_model.predict(X_test)
y_prob_test_gb = best_gb_model.predict_proba(X_test)[:, 1]

def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics

metrics_test = bootstrap_metrics(best_gb_model, X_test, y_test)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

conf_matrix_gb = confusion_matrix(y_test, y_pred_test_gb)
accuracy_gb = accuracy_score(y_test, y_pred_test_gb)
report_gb = classification_report(y_test, y_pred_test_gb, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix_gb)
print("Accuracy:", accuracy_gb)
print("Classification Report (Test Data):\n", report_gb)

kf_gb = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_prob_cv_gb = cross_val_predict(best_gb_model, X_train, y_train, cv=kf_gb, method='predict_proba')[:, 1]
y_pred_cv_gb = cross_val_predict(best_gb_model, X_train, y_train, cv=kf_gb)

accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_gb.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_gb_model.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_gb_model.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))

accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")

conf_matrix_cv_gb = confusion_matrix(y_train, y_pred_cv_gb)
accuracy_cv_gb = accuracy_score(y_train, y_pred_cv_gb)
report_cv_gb = classification_report(y_train, y_pred_cv_gb, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv_gb)
print("Cross-Validation Accuracy:", accuracy_cv_gb)
print("Classification Report (Cross-Validation):\n", report_cv_gb)

fpr_gb, tpr_gb, _ = roc_curve(y_test, y_prob_test_gb, pos_label=1)
roc_auc_gb = auc(fpr_gb, tpr_gb)
fpr_cv_gb, tpr_cv_gb, _ = roc_curve(y_train, y_prob_cv_gb, pos_label=1)
roc_auc_cv_gb = auc(fpr_cv_gb, tpr_cv_gb)


def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=1)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

fpr_test, tpr_test, _ = roc_curve(y_test, y_prob_test_gb, pos_label=1)
auc_test = auc(fpr_test, tpr_test)
auc_scores_test_gb = bootstrap_auc(best_gb_model, X_test, y_test)

auc_ci_test_gb = np.percentile(auc_scores_test_gb, [2.5, 97.5])

print(f"Test Data AUC: {auc_test} (95% CI: {auc_ci_test_gb})")


def auc_cross_val(model, X, y, cv, pos_label=1):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=1)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

auc_scores_cv_gb = auc_cross_val(best_gb_model, X_train, y_train, kf_gb, pos_label=1)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

auc_ci_cv_gb = ci95(auc_scores_cv_gb)

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_gb)} (95% CI: {auc_ci_cv_gb})")

###Calculation of ROC Curve metrics and graph plotting (Test data and cross-validation)
fpr_gb, tpr_gb, _ = roc_curve(y_test, y_prob_test_gb)
roc_auc_gb = auc(fpr_gb, tpr_gb)

plt.figure(figsize=(8, 8))
plt.plot(fpr_gb, tpr_gb, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_gb))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Test Data')
plt.legend(loc='lower right')
plt.show()

# Cross-validation data
fpr_cv_gb, tpr_cv_gb, _ = roc_curve(y_train, y_prob_cv_gb)
roc_auc_cv_gb = auc(fpr_cv_gb, tpr_cv_gb)

plt.figure(figsize=(8, 8))
plt.plot(fpr_cv_gb, tpr_cv_gb, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_cv_gb))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Cross-Validation')
plt.legend(loc='lower right')
plt.show()

##FEATURE IMPORTANCE GRADIENT BOOSTING CLASSIFIER
X = df.drop('Risk', axis=1)
features=[]
for columns in X.columns:
    features.append(columns)
imp_features = best_gb_model.feature_importances_
importances = best_gb_model.feature_importances_

feature_importance_gb = pd.DataFrame({'Feature': features, 'Importance': importances})

feature_importance_sorted = feature_importance_gb.sort_values('Importance', ascending=True)
colors = plt.cm.magma(np.linspace(0.2, 1, len(feature_importance_sorted)))
plt.figure(figsize=(10, 6))
bars = plt.barh(feature_importance_sorted['Feature'], feature_importance_sorted['Importance'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')

for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')

plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()
plt.show()

plt.savefig('figimpKNN.jpg', dpi=300, bbox_inches='tight')

"""**LOGISTIC REGRESSION**"""

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, KFold, cross_val_predict, train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, recall_score, f1_score, roc_curve, auc
from sklearn.utils import resample

param_grid = {
    'C': [0.001, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1, 0.2, 0.3, 0.5, 1, 5, 10, 100],
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'max_iter': [50, 100, 300, 500, 1000],
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
    'l1_ratio': [0.2, 0.4, 0.6, 0.8]
}

logreg_model = LogisticRegression(random_state=RANDOM_STATE)
grid_search = GridSearchCV(estimator=logreg_model, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_logreg_model = grid_search.best_estimator_
best_logreg_model.fit(X_train, y_train)
y_pred_test_logreg = best_logreg_model.predict(X_test)
y_prob_test_logreg = best_logreg_model.predict_proba(X_test)[:, 1]

def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics


metrics_test = bootstrap_metrics(best_logreg_model, X_test, y_test)


def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

conf_matrix_logreg = confusion_matrix(y_test, y_pred_test_logreg)
accuracy_logreg = accuracy_score(y_test, y_pred_test_logreg)
report_logreg = classification_report(y_test, y_pred_test_logreg, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix_logreg)
print("Accuracy:", accuracy_logreg)
print("Classification Report (Test Data):\n", report_logreg)


kf_logreg = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_prob_cv_logreg = cross_val_predict(best_logreg_model, X_train, y_train, cv=kf_logreg, method='predict_proba')[:, 1]
y_pred_cv_logreg = cross_val_predict(best_logreg_model, X_train, y_train, cv=kf_logreg)


accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_logreg.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_logreg_model.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_logreg_model.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))


accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")


conf_matrix_cv_logreg = confusion_matrix(y_train, y_pred_cv_logreg)
accuracy_cv_logreg = accuracy_score(y_train, y_pred_cv_logreg)
report_cv_logreg = classification_report(y_train, y_pred_cv_logreg, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv_logreg)
print("Cross-Validation Accuracy:", accuracy_cv_logreg)
print("Classification Report (Cross-Validation):\n", report_cv_logreg)


fpr_logreg, tpr_logreg, _ = roc_curve(y_test, y_prob_test_logreg, pos_label=1)
roc_auc_logreg = auc(fpr_logreg, tpr_logreg)
fpr_cv_logreg, tpr_cv_logreg, _ = roc_curve(y_train, y_prob_cv_logreg, pos_label=1)
roc_auc_cv_logreg = auc(fpr_cv_logreg, tpr_cv_logreg)


def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=1)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

# Calcular AUC para dados de teste e CI 95%
fpr_test, tpr_test, _ = roc_curve(y_test, y_prob_test_logreg, pos_label=1)
auc_test_lr = auc(fpr_test, tpr_test)
auc_scores_test_lr = bootstrap_auc(best_logreg_model, X_test, y_test)

auc_ci_test_lr = np.percentile(auc_scores_test_lr, [2.5, 97.5])

print(f"Test Data AUC: {auc_test_lr} (95% CI: {auc_ci_test_lr})")

# Função para calcular AUC com validação cruzada
def auc_cross_val(model, X, y, cv, pos_label=1):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=1)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

# Calcular AUC para validação cruzada
auc_scores_cv_lr = auc_cross_val(best_logreg_model, X_train, y_train, kf_logreg, pos_label=1)

# Calcular IC95% para AUC na validação cruzada
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

auc_ci_cv_lr = ci95(auc_scores_cv_lr)

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_lr)} (95% CI: {auc_ci_cv_lr})")

#Calculation of ROC Curve metrics and graph plotting
fpr_logreg, tpr_logreg, _ = roc_curve(y_test, y_prob_test_logreg)
roc_auc_logreg = auc(fpr_logreg, tpr_logreg)

plt.figure(figsize=(8, 8))
plt.plot(fpr_logreg, tpr_logreg, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_logreg))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Test Data')
plt.legend(loc='lower right')
plt.show()
fpr_cv_logreg, tpr_cv_logreg, _ = roc_curve(y_train, y_prob_cv_logreg)
roc_auc_cv_logreg = auc(fpr_cv_logreg, tpr_cv_logreg)
plt.figure(figsize=(8, 8))
plt.plot(fpr_cv_logreg, tpr_cv_logreg, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_cv_logreg))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Cross-Validation')
plt.legend(loc='lower right')
plt.show()

#FEATURE IMPORTANCE LOGISTIC REGRESSION
model = best_logreg_model.fit(X_train, y_train)
coefficients = model.coef_[0]
feature_importance_lr = pd.DataFrame({'Feature': X.columns, 'Importance': np.abs(coefficients)})
feature_importance_lr = feature_importance_lr.sort_values('Importance', ascending=True)
colors = plt.cm.plasma(np.linspace(0.2, 1, len(feature_importance_lr)))
plt.figure(figsize=(10, 6))
bars = plt.barh(feature_importance_lr['Feature'], feature_importance_lr['Importance'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

plt.show()

"""# SVM"""

# Definir o parâmetro do grid search
param_grid = {
    'C': [0.001, 0.1, 0.9, 1.5, 2.3, 23, 50, 100],
    'kernel': ['linear', 'rbf'],
    'gamma': ['auto', 'scale']
}

# Configurar o modelo e a validação cruzada
svc_model = SVC(random_state=RANDOM_STATE, probability=True)
grid_search = GridSearchCV(estimator=svc_model, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_svc_model = grid_search.best_estimator_
best_svc_model.fit(X_train, y_train)
y_pred_test_svc = best_svc_model.predict(X_test)
y_prob_test_svc = best_svc_model.predict_proba(X_test)[:, 1]

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test = bootstrap_metrics(best_svc_model, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

# Exibir os resultados padrão para os dados de teste
conf_matrix_svc = confusion_matrix(y_test, y_pred_test_svc)
accuracy_svc = accuracy_score(y_test, y_pred_test_svc)
report_svc = classification_report(y_test, y_pred_test_svc, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix_svc)
print("Accuracy:", accuracy_svc)
print("Classification Report (Test Data):\n", report_svc)

# Configurar a validação cruzada
kf_svc = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_prob_cv_svc = cross_val_predict(best_svc_model, X_train, y_train, cv=kf_svc, method='predict_proba')[:, 1]
y_pred_cv_svc = cross_val_predict(best_svc_model, X_train, y_train, cv=kf_svc)

# Calcular as métricas para cada fold da validação cruzada
accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_svc.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_svc_model.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_svc_model.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))

# Calcular os IC95% para validação cruzada
accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")

# Exibir os resultados padrão para validação cruzada
conf_matrix_cv_svc = confusion_matrix(y_train, y_pred_cv_svc)
accuracy_cv_svc = accuracy_score(y_train, y_pred_cv_svc)
report_cv_svc = classification_report(y_train, y_pred_cv_svc, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv_svc)
print("Cross-Validation Accuracy:", accuracy_cv_svc)
print("Classification Report (Cross-Validation):\n", report_cv_svc)

# IC95% PARA AUC
fpr_svc, tpr_svc, _ = roc_curve(y_test, y_prob_test_svc, pos_label=1)
roc_auc_svc = auc(fpr_svc, tpr_svc)
fpr_cv_svc, tpr_cv_svc, _ = roc_curve(y_train, y_prob_cv_svc, pos_label=1)
roc_auc_cv_svc = auc(fpr_cv_svc, tpr_cv_svc)

# Função para calcular AUC com bootstrapping
def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=1)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

# Calcular AUC para dados de teste e CI 95%
fpr_test, tpr_test, _ = roc_curve(y_test, y_prob_test_svc, pos_label=1)
auc_test_sv = auc(fpr_test, tpr_test)
auc_scores_test_sv = bootstrap_auc(best_svc_model, X_test, y_test)

auc_ci_test_sv = np.percentile(auc_scores_test_sv, [2.5, 97.5])

print(f"Test Data AUC: {auc_test_sv} (95% CI: {auc_ci_test_sv})")

# Função para calcular AUC com validação cruzada
def auc_cross_val(model, X, y, cv, pos_label=1):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=pos_label)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

# Calcular AUC para validação cruzada
auc_scores_cv_sv = auc_cross_val(best_svc_model, X_train, y_train, kf_svc, pos_label=1)

# Calcular IC95% para AUC na validação cruzada
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

auc_ci_cv_sv = ci95(auc_scores_cv_sv)

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_sv)} (95% CI: {auc_ci_cv_sv})")

#Calculation of ROC Curve metrics and graph plotting
fpr_svc, tpr_svc, _ = roc_curve(y_test, y_prob_test_svc)
roc_auc_svc = auc(fpr_svc, tpr_svc)

plt.figure(figsize=(8, 8))
plt.plot(fpr_svc, tpr_svc, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_svc))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Test Data')
plt.legend(loc='lower right')
plt.show()
# Plotar a curva ROC para a validação cruzada
fpr_cv_svc, tpr_cv_svc, _ = roc_curve(y_train, y_prob_cv_svc)
roc_auc_cv_svc = auc(fpr_cv_svc, tpr_cv_svc)

plt.figure(figsize=(8, 8))
plt.plot(fpr_cv_svc, tpr_cv_svc, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_cv_svc))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Cross-Validation')
plt.legend(loc='lower right')
plt.show()

"""# KNN"""

# Definir o parâmetro do grid search
param_grid = {
    'n_neighbors': [1, 3, 5, 7, 10, 15, 100, 1000],
    'weights': ['uniform', 'distance'],
    'p': [0.001, 0.1, 1, 3, 5, 7, 10, 15, 100, 1000],
    'leaf_size': [0.001, 0.1, 1, 3, 5, 7, 10, 15, 100, 1000]
}

# Configurar o modelo e a validação cruzada
knn_model = KNeighborsClassifier()
grid_search = GridSearchCV(estimator=knn_model, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_knn_model = grid_search.best_estimator_
best_knn_model.fit(X_train, y_train)
y_pred_test_knn = best_knn_model.predict(X_test)
y_prob_test_knn = best_knn_model.predict_proba(X_test)[:, 1]

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test = bootstrap_metrics(best_knn_model, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

# Exibir os resultados padrão para os dados de teste
conf_matrix_knn = confusion_matrix(y_test, y_pred_test_knn)
accuracy_knn = accuracy_score(y_test, y_pred_test_knn)
report_knn = classification_report(y_test, y_pred_test_knn, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix_knn)
print("Accuracy:", accuracy_knn)
print("Classification Report (Test Data):\n", report_knn)

# Configurar a validação cruzada
kf_knn = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_prob_cv_knn = cross_val_predict(best_knn_model, X_train, y_train, cv=kf_knn, method='predict_proba')[:, 1]
y_pred_cv_knn = cross_val_predict(best_knn_model, X_train, y_train, cv=kf_knn)

# Calcular as métricas para cada fold da validação cruzada
accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_knn.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_knn_model.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_knn_model.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))

# Calcular os IC95% para validação cruzada
accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")

# Exibir os resultados padrão para validação cruzada
conf_matrix_cv_knn = confusion_matrix(y_train, y_pred_cv_knn)
accuracy_cv_knn = accuracy_score(y_train, y_pred_cv_knn)
report_cv_knn = classification_report(y_train, y_pred_cv_knn, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv_knn)
print("Cross-Validation Accuracy:", accuracy_cv_knn)
print("Classification Report (Cross-Validation):\n", report_cv_knn)

# IC95% PARA AUC
fpr_knn, tpr_knn, _ = roc_curve(y_test, y_prob_test_knn, pos_label=1)
roc_auc_knn = auc(fpr_knn, tpr_knn)
fpr_cv_knn, tpr_cv_knn, _ = roc_curve(y_train, y_prob_cv_knn, pos_label=1)
roc_auc_cv_knn = auc(fpr_cv_knn, tpr_cv_knn)

# Função para calcular AUC com bootstrapping
def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=1)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

# Calcular AUC para dados de teste e CI 95%
fpr_test, tpr_test, _ = roc_curve(y_test, y_prob_test_knn, pos_label=1)
auc_test_knn = auc(fpr_test, tpr_test)
auc_scores_test_knn = bootstrap_auc(best_knn_model, X_test, y_test)

auc_ci_test_knn = np.percentile(auc_scores_test_knn, [2.5, 97.5])

print(f"Test Data AUC: {auc_test_knn} (95% CI: {auc_ci_test_knn})")

# Função para calcular AUC com validação cruzada
def auc_cross_val(model, X, y, cv, pos_label=1):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=pos_label)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

# Calcular AUC para validação cruzada
auc_scores_cv_knn = auc_cross_val(best_knn_model, X_train, y_train, kf_knn, pos_label=1)

# Calcular IC95% para AUC na validação cruzada
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

auc_ci_cv_knn = ci95(auc_scores_cv_knn)

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_knn)} (95% CI: {auc_ci_cv_knn})")

#Calculation of ROC Curve metrics and graph plotting
fpr_knn, tpr_knn, _ = roc_curve(y_test, y_prob_test_knn)
roc_auc_knn = auc(fpr_knn, tpr_knn)

plt.figure(figsize=(8, 8))
plt.plot(fpr_knn, tpr_knn, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_knn))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Test Data')
plt.legend(loc='lower right')
plt.show()

fpr_cv_knn, tpr_cv_knn, _ = roc_curve(y_train, y_prob_cv_knn)
roc_auc_cv_knn = auc(fpr_cv_knn, tpr_cv_knn)

plt.figure(figsize=(8, 8))
plt.plot(fpr_cv_knn, tpr_cv_knn, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_cv_knn))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Cross-Validation')
plt.legend(loc='lower right')
plt.show()

"""# MLP CLASSIFIER"""

# Definir o parâmetro do grid search
param_grid = {
    'hidden_layer_sizes': [(10,), (100,), (1000,)],
    'alpha': [0.01, 0.1, 1.0],
    'learning_rate_init': [0.01, 0.1, 1],
    'activation': ['relu', 'logistic', 'tanh'],
    'max_iter': [50, 100, 1000],
    'solver': ['lbfgs', 'sgd', 'adam']
}

# Configurar o modelo e a validação cruzada
mlp_model = MLPClassifier(random_state=RANDOM_STATE)
grid_search = GridSearchCV(estimator=mlp_model, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_mlp_model = grid_search.best_estimator_
best_mlp_model.fit(X_train, y_train)
y_pred_test_mlp = best_mlp_model.predict(X_test)
y_prob_test_mlp = best_mlp_model.predict_proba(X_test)[:, 1]

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test = bootstrap_metrics(best_mlp_model, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

# Exibir os resultados padrão para os dados de teste
conf_matrix_mlp = confusion_matrix(y_test, y_pred_test_mlp)
accuracy_mlp = accuracy_score(y_test, y_pred_test_mlp)
report_mlp = classification_report(y_test, y_pred_test_mlp, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix_mlp)
print("Accuracy:", accuracy_mlp)
print("Classification Report (Test Data):\n", report_mlp)

# Configurar a validação cruzada
kf_mlp = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_prob_cv_mlp = cross_val_predict(best_mlp_model, X_train, y_train, cv=kf_mlp, method='predict_proba')[:, 1]
y_pred_cv_mlp = cross_val_predict(best_mlp_model, X_train, y_train, cv=kf_mlp)

# Calcular as métricas para cada fold da validação cruzada
accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_mlp.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_mlp_model.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_mlp_model.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))

# Calcular os IC95% para validação cruzada
accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")

# Exibir os resultados padrão para validação cruzada
conf_matrix_cv_mlp = confusion_matrix(y_train, y_pred_cv_mlp)
accuracy_cv_mlp = accuracy_score(y_train, y_pred_cv_mlp)
report_cv_mlp = classification_report(y_train, y_pred_cv_mlp, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv_mlp)
print("Cross-Validation Accuracy:", accuracy_cv_mlp)
print("Classification Report (Cross-Validation):\n", report_cv_mlp)

# IC95% PARA AUC
fpr_mlp, tpr_mlp, _ = roc_curve(y_test, y_prob_test_mlp, pos_label=1)
roc_auc_mlp = auc(fpr_mlp, tpr_mlp)
fpr_cv_mlp, tpr_cv_mlp, _ = roc_curve(y_train, y_prob_cv_mlp, pos_label=1)
roc_auc_cv_mlp = auc(fpr_cv_mlp, tpr_cv_mlp)

# Função para calcular AUC com bootstrapping
def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=1)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

# Calcular AUC para dados de teste e CI 95%
auc_scores_test_mlp = bootstrap_auc(best_mlp_model, X_test, y_test)
auc_ci_test_mlp = ci95(auc_scores_test_mlp)
print(f"Test Data AUC: {roc_auc_mlp} (95% CI: {auc_ci_test_mlp})")

# Função para calcular AUC com validação cruzada
def auc_cross_val(model, X, y, cv, pos_label=1):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=1)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

# Calcular AUC para validação cruzada
auc_scores_cv_mlp = auc_cross_val(best_mlp_model, X_train, y_train, kf_mlp, pos_label=1)

# Calcular IC95% para AUC na validação cruzada
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

auc_ci_cv_mlp = ci95(auc_scores_cv_mlp)

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_mlp)} (95% CI: {auc_ci_cv_mlp})")

#Calculation of ROC Curve metrics and graph plotting
fpr_mlp, tpr_mlp, _ = roc_curve(y_test, y_prob_test_mlp)
roc_auc_mlp = auc(fpr_mlp, tpr_mlp)
plt.figure(figsize=(8, 8))
plt.plot(fpr_mlp, tpr_mlp, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_mlp))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Test Data')
plt.legend(loc='lower right')
plt.show()

fpr_cv_mlp, tpr_cv_mlp, _ = roc_curve(y_train, y_prob_cv_mlp)
roc_auc_cv_mlp = auc(fpr_cv_mlp, tpr_cv_mlp)

plt.figure(figsize=(8, 8))
plt.plot(fpr_cv_mlp, tpr_cv_mlp, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_cv_mlp))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Cross-Validation')
plt.legend(loc='lower right')
plt.show()

"""# DECISION TREE"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, KFold, cross_val_predict, train_test_split
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             classification_report, confusion_matrix, roc_curve, auc)
from sklearn.utils import resample
import numpy as np

# Definir os parâmetros do grid search para DT
param_grid = {
    'criterion': ['gini', 'entropy'],
    'splitter': ['best', 'random'],
    'max_depth': [None, 5, 10, 15]
}

# Configurar o modelo e a validação cruzada
tree_clf = DecisionTreeClassifier(random_state=RANDOM_STATE)
grid_search = GridSearchCV(estimator=tree_clf, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_tree_clf = grid_search.best_estimator_
best_tree_clf.fit(X_train, y_train)
y_pred_test_dt = best_tree_clf.predict(X_test)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test = bootstrap_metrics(best_tree_clf, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

# Exibir os resultados padrão para os dados de teste
conf_matrix_dt = confusion_matrix(y_test, y_pred_test_dt)
accuracy_dt = accuracy_score(y_test, y_pred_test_dt)
report_dt = classification_report(y_test, y_pred_test_dt, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix_dt)
print("Accuracy:", accuracy_dt)
print("Classification Report (Test Data):\n", report_dt)

# Configurar a validação cruzada
kf_dt = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_dt = cross_val_predict(best_tree_clf, X_train, y_train, cv=kf_dt)

# Calcular as métricas para cada fold da validação cruzada
accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_dt.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_tree_clf.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_tree_clf.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))

# Calcular os IC95% para validação cruzada
accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")

# Exibir os resultados padrão para validação cruzada
conf_matrix_cv_dt = confusion_matrix(y_train, y_pred_cv_dt)
accuracy_cv_dt = accuracy_score(y_train, y_pred_cv_dt)
report_cv_dt = classification_report(y_train, y_pred_cv_dt, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv_dt)
print("Cross-Validation Accuracy:", accuracy_cv_dt)
print("Classification Report (Cross-Validation):\n", report_cv_dt)

# IC95% PARA AUC
y_prob_test_dt = best_tree_clf.predict_proba(X_test)[:, 1]
y_prob_cv_dt = cross_val_predict(best_tree_clf, X_train, y_train, cv=kf_dt, method='predict_proba')[:, 1]

fpr_dt, tpr_dt, _ = roc_curve(y_test, y_prob_test_dt, pos_label=1)
roc_auc_dt = auc(fpr_dt, tpr_dt)
fpr_cv_dt, tpr_cv_dt, _ = roc_curve(y_train, y_prob_cv_dt, pos_label=1)
roc_auc_cv_dt = auc(fpr_cv_dt, tpr_cv_dt)

# Função para calcular AUC com bootstrapping
def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=1)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

# Calcular AUC para dados de teste e CI 95%
auc_scores_test_dt = bootstrap_auc(best_tree_clf, X_test, y_test)
auc_ci_test_dt = np.percentile(auc_scores_test_dt, [2.5, 97.5])

print(f"Test Data AUC: {roc_auc_dt} (95% CI: {auc_ci_test_dt})")

# Função para calcular AUC com validação cruzada
def auc_cross_val(model, X, y, cv, pos_label=1):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=pos_label)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

# Calcular AUC para validação cruzada
auc_scores_cv_dt = auc_cross_val(best_tree_clf, X_train, y_train, kf_dt, pos_label=1)

# Calcular IC95% para AUC na validação cruzada
auc_ci_cv_dt = ci95(auc_scores_cv_dt)

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_dt)} (95% CI: {auc_ci_cv_dt})")

#Calculation of ROC Curve metrics and graph plotting

from sklearn.model_selection import StratifiedKFold
def plot_roc_curve(fpr, tpr, auc_score, title='Receiver Operating Characteristic'):
    plt.figure(figsize=(8, 8))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc_score))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend(loc='lower right')
    plt.show()
y_prob_test_dt = best_tree_clf.predict_proba(X_test)[:, 1]
fpr_dt, tpr_dt, _ = roc_curve(y_test, y_prob_test_dt)
roc_auc_dt = auc(fpr_dt, tpr_dt)
plot_roc_curve(fpr_dt, tpr_dt, roc_auc_dt, title='ROC - Decision Tree - Test Data')
kf_dt = StratifiedKFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_prob_cv_dt = cross_val_predict(best_tree_clf, X_train, y_train, cv=kf_dt, method='predict_proba')[:, 1]
fpr_cv_dt, tpr_cv_dt, _ = roc_curve(y_train, y_prob_cv_dt)
roc_auc_cv_dt = auc(fpr_cv_dt, tpr_cv_dt)
plot_roc_curve(fpr_cv_dt, tpr_cv_dt, roc_auc_cv_dt, title='ROC - Decision Tree - Cross-Validation')

#FEATURE IMPORTANCE DECISION TREE
X_2 = []
X_2
features_2 =[]
imp_features_dt2 = []
df_imp_features_dt2 = []
X_2 = df.drop('Risk', axis=1)
features_2 = []

for column in X_2.columns:
    features_2.append(column)

imp_features_dt2 = best_tree_clf.feature_importances_
df_imp_features_dt2 = pd.DataFrame({"features": features_2, "weights": imp_features_dt2})
df_imp_features_dt2_sorted = df_imp_features_dt2.sort_values(by='weights', ascending=True)
plt.figure(figsize=(10, 6))
colors = plt.cm.cividis(np.linspace(0.2, 1, len(df_imp_features_dt2_sorted)))
bars = plt.barh(df_imp_features_dt2_sorted['features'], df_imp_features_dt2_sorted['weights'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

plt.show()

"""# RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, KFold, cross_val_predict
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             classification_report, confusion_matrix, roc_curve, auc)
from sklearn.utils import resample
import numpy as np

# Definir os parâmetros do grid search para RF
param_grid = {
    'n_estimators': [5, 50, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 10, 15],
    'min_samples_leaf': [1, 4, 6],
    'max_features': ['auto', 'sqrt'],
    'criterion': ['gini', 'entropy']
}

# Configurar o modelo e a validação cruzada
rf_model = RandomForestClassifier(random_state=RANDOM_STATE)
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_rf_model = grid_search.best_estimator_
best_rf_model.fit(X_train, y_train)
y_pred_test_rf = best_rf_model.predict(X_test)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test = bootstrap_metrics(best_rf_model, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

# Exibir os resultados padrão para os dados de teste
conf_matrix_rf = confusion_matrix(y_test, y_pred_test_rf)
accuracy_rf = accuracy_score(y_test, y_pred_test_rf)
report_rf = classification_report(y_test, y_pred_test_rf, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix_rf)
print("Accuracy:", accuracy_rf)
print("Classification Report (Test Data):\n", report_rf)

# Configurar a validação cruzada
kf_rf = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_rf = cross_val_predict(best_rf_model, X_train, y_train, cv=kf_rf)

# Calcular as métricas para cada fold da validação cruzada
accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_rf.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_rf_model.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_rf_model.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))

# Calcular os IC95% para validação cruzada
accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")

# Exibir os resultados padrão para validação cruzada
conf_matrix_cv_rf = confusion_matrix(y_train, y_pred_cv_rf)
accuracy_cv_rf = accuracy_score(y_train, y_pred_cv_rf)
report_cv_rf = classification_report(y_train, y_pred_cv_rf, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv_rf)
print("Cross-Validation Accuracy:", accuracy_cv_rf)
print("Classification Report (Cross-Validation):\n", report_cv_rf)

# IC95% PARA AUC
y_prob_test_rf = best_rf_model.predict_proba(X_test)[:, 1]
y_prob_cv_rf = cross_val_predict(best_rf_model, X_train, y_train, cv=kf_rf, method='predict_proba')[:, 1]

fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_test_rf, pos_label=1)
roc_auc_rf = auc(fpr_rf, tpr_rf)
fpr_cv_rf, tpr_cv_rf, _ = roc_curve(y_train, y_prob_cv_rf, pos_label=1)
roc_auc_cv_rf = auc(fpr_cv_rf, tpr_cv_rf)

# Função para calcular AUC com bootstrapping
def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=1)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

# Calcular AUC para dados de teste e CI 95%
auc_scores_test_rf = bootstrap_auc(best_rf_model, X_test, y_test)
auc_ci_test_rf = np.percentile(auc_scores_test_rf, [2.5, 97.5])

print(f"Test Data AUC: {roc_auc_rf} (95% CI: {auc_ci_test_rf})")

# Função para calcular AUC com validação cruzada
def auc_cross_val(model, X, y, cv, pos_label=1):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=pos_label)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

# Calcular AUC para validação cruzada
auc_scores_cv_rf = auc_cross_val(best_rf_model, X_train, y_train, kf_rf, pos_label=1)

# Calcular IC95% para AUC na validação cruzada
auc_ci_cv_rf = ci95(auc_scores_cv_rf)

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_rf)} (95% CI: {auc_ci_cv_rf})")

#Calculation of ROC Curve metrics and graph plotting

fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_test_rf)
roc_auc_rf = auc(fpr_rf, tpr_rf)

plt.figure(figsize=(8, 8))
plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_rf))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Test Data')
plt.legend(loc='lower right')
plt.show()
fpr_cv_rf, tpr_cv_rf, _ = roc_curve(y_train, y_prob_cv_rf)
roc_auc_cv_rf = auc(fpr_cv_rf, tpr_cv_rf)

plt.figure(figsize=(8, 8))
plt.plot(fpr_cv_rf, tpr_cv_rf, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_cv_rf))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Cross-Validation')
plt.legend(loc='lower right')
plt.show()

#FEATURE IMPORTANCE RANDOM FOREST CLASSIFIER

X_3 = df.drop('Risk', axis=1)
features_3 = []

for column in X_3.columns:
    features_3.append(column)

imp_features_dt3 = best_rf_model.feature_importances_
df_imp_features_dt3 = pd.DataFrame({"features": features_3, "weights": imp_features_dt3})
df_imp_features_dt3_sorted = df_imp_features_dt3.sort_values(by='weights', ascending=True)
plt.figure(figsize=(10, 6))
colors = plt.cm.inferno(np.linspace(0.2, 1, len(df_imp_features_dt3_sorted)))
bars = plt.barh(df_imp_features_dt3_sorted['features'], df_imp_features_dt3_sorted['weights'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

plt.show()

"""#**Adaboosting**"""

import numpy as np
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, KFold, cross_val_predict
from sklearn.metrics import (confusion_matrix, accuracy_score, classification_report,
                             precision_score, recall_score, f1_score, roc_curve, auc)
from sklearn.utils import resample

# Definindo os hiperparâmetros para o AdaBoost
param_grid = {
    'n_estimators': [5, 50, 200],
    'learning_rate': [0.01, 0.1, 1.0],
    'base_estimator__max_depth': [1, 2, 3],  # Usando DecisionTreeClassifier como base
    'base_estimator__min_samples_split': [2, 10, 15],
    'base_estimator__min_samples_leaf': [1, 4, 6]
}

# Criando o classificador base
base_clf = DecisionTreeClassifier(random_state=RANDOM_STATE)

# Criando o classificador AdaBoost com o classificador base
ada_clf = AdaBoostClassifier(base_estimator=base_clf, random_state=RANDOM_STATE)

# GridSearchCV para encontrar os melhores hiperparâmetros
grid_search = GridSearchCV(estimator=ada_clf, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Exibindo os melhores parâmetros e a melhor pontuação
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

# Treinando o modelo com os melhores parâmetros
best_ada_clf = grid_search.best_estimator_
best_ada_clf.fit(X_train, y_train)

# Fazendo previsões no conjunto de teste
y_pred_test_ada = best_ada_clf.predict(X_test)
y_prob_test_ada = best_ada_clf.predict_proba(X_test)[:, 1]

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test = bootstrap_metrics(best_ada_clf, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

# Exibir os resultados padrão para os dados de teste
conf_matrix = confusion_matrix(y_test, y_pred_test_ada)
accuracy_ada = accuracy_score(y_test, y_pred_test_ada)
report = classification_report(y_test, y_pred_test_ada, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix)
print("Accuracy:", accuracy_ada)
print("Classification Report (Test Data):\n", report)

# Configurar a validação cruzada
kf_ada = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_ada = cross_val_predict(best_ada_clf, X_train, y_train, cv=kf_ada)
y_prob_cv_ada = cross_val_predict(best_ada_clf, X_train, y_train, cv=kf_ada, method='predict_proba')[:, 1]

# Calcular as métricas para cada fold da validação cruzada
accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_ada.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_ada_clf.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_ada_clf.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))

# Calcular os IC95% para validação cruzada
accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")

# Exibir os resultados padrão para validação cruzada
conf_matrix_cv = confusion_matrix(y_train, y_pred_cv_ada)
accuracy_cv_ada = accuracy_score(y_train, y_pred_cv_ada)
report_cv = classification_report(y_train, y_pred_cv_ada, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv)
print("Cross-Validation Accuracy:", accuracy_cv_ada)
print("Classification Report (Cross-Validation):\n", report_cv)

# IC95% PARA AUC
fpr_test, tpr_test, _ = roc_curve(y_test, y_prob_test_ada, pos_label=1)
roc_auc_test_ada = auc(fpr_test, tpr_test)
fpr_cv, tpr_cv, _ = roc_curve(y_train, y_prob_cv_ada, pos_label=1)
roc_auc_cv = auc(fpr_cv, tpr_cv)

# Função para calcular AUC com bootstrapping
def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=1)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

# Calcular AUC para dados de teste e CI 95%
auc_scores_test_ada = bootstrap_auc(best_ada_clf, X_test, y_test)
auc_ci_test_ada = np.percentile(auc_scores_test_ada, [2.5, 97.5])

print(f"Test Data AUC: {roc_auc_test_ada} (95% CI: {auc_ci_test_ada})")

# Função para calcular AUC com validação cruzada
def auc_cross_val(model, X, y, cv, pos_label=1):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=pos_label)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

# Calcular AUC para validação cruzada
auc_scores_cv_ada = auc_cross_val(best_ada_clf, X_train, y_train, kf_ada)

# Calcular IC95% para AUC na validação cruzada
auc_ci_cv_ada = np.percentile(auc_scores_cv_ada, [2.5, 97.5])

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_ada)} (95% CI: {auc_ci_cv_ada})")

fpr_ada, tpr_ada, _ = roc_curve(y_test, y_prob_test_ada)
roc_auc_ada = auc(fpr_ada, tpr_ada)

plt.figure(figsize=(8, 8))
plt.plot(fpr_ada, tpr_ada, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_ada))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Test Data')
plt.legend(loc='lower right')
plt.show()
fpr_cv_ada, tpr_cv_ada, _ = roc_curve(y_train, y_prob_cv_ada)
roc_auc_cv_ada = auc(fpr_cv_ada, tpr_cv_ada)

plt.figure(figsize=(8, 8))
plt.plot(fpr_cv_ada, tpr_cv_ada, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_cv_ada))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Cross-Validation')
plt.legend(loc='lower right')
plt.show()

#FEATURE IMPORTANCE - ADABOOSTING

X_3 = df.drop('Risk', axis=1)
features_3 = X_3.columns.tolist()
imp_features_ada = best_ada_clf.feature_importances_

df_imp_features_ada = pd.DataFrame({"features": features_3, "weights": imp_features_ada})
df_imp_features_ada_sorted = df_imp_features_ada.sort_values(by='weights', ascending=True)

plt.figure(figsize=(10, 6))
colors = plt.cm.inferno(np.linspace(0.2, 1, len(df_imp_features_ada_sorted)))
bars = plt.barh(df_imp_features_ada_sorted['features'], df_imp_features_ada_sorted['weights'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')

for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')

plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

plt.show()

"""# Plot-ROC"""

plt.style.use('seaborn-whitegrid')
fig, axs = plt.subplots(1, 2, figsize=(20, 8))

axs[0].plot(fpr_logreg, tpr_logreg, color='red', lw=2, label='Logistic Regression (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_logreg, auc_ci_test_lr[0], auc_ci_test_lr[1]))
axs[0].plot(fpr_gb, tpr_gb, color='orange', lw=2, label='Gradient Boosting (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_gb, auc_ci_test_gb[0], auc_ci_test_gb[1]))
axs[0].plot(fpr_rf, tpr_rf, color='deeppink', lw=2, label='Random Forest (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_rf, auc_ci_test_rf[0], auc_ci_test_rf[1]))
axs[0].plot(fpr_knn, tpr_knn, color='blue', lw=2, label='K Nearest Neighbors (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_knn, auc_ci_test_knn[0], auc_ci_test_knn[1]))
axs[0].plot(fpr_mlp, tpr_mlp, color='purple', lw=2, label='Multilayer Perceptron (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_mlp, auc_ci_test_mlp[0], auc_ci_test_mlp[1]))
axs[0].plot(fpr_logreg, tpr_logreg, color='red', lw=2, label='Decision Tree (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_dt, auc_ci_test_dt[0], auc_ci_test_dt[1]))
axs[0].plot(fpr_ada, tpr_ada, color='red', lw=2, label='Adaboost Classifier (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_ada, auc_ci_test_ada[0], auc_ci_test_ada[1]))
axs[0].plot(fpr_svc, tpr_svc, color='green', lw=2, label='Support Vector Machine (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_svc, auc_ci_test_sv[0], auc_ci_test_sv[1]))




axs[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
axs[0].set_xlim([0.0, 1.0])
axs[0].set_ylim([0.0, 1.05])
axs[0].set_xlabel('False Positive Rate')
axs[0].set_ylabel('True Positive Rate')
axs[0].set_title('Receiver Operating Characteristic (ROC) Curve')
axs[0].legend(loc="lower right")



axs[1].plot(fpr_cv_gb, tpr_cv_gb, color='orange', lw=2, label='Gradient Boosting (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_gb, auc_ci_cv_gb[0], auc_ci_cv_gb[1]))
axs[1].plot(fpr_cv_rf, tpr_cv_rf, color='deeppink', lw=2, label='Random Forest (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_rf, auc_ci_cv_rf[0], auc_ci_cv_rf[1]))
axs[1].plot(fpr_cv_ada, tpr_cv_ada, color='orange', lw=2, label='Adaboost Classifier (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_ada, auc_ci_cv_ada[0], auc_ci_cv_ada[1]))
axs[1].plot(fpr_cv_knn, tpr_cv_knn, color='blue', lw=2, label='K Nearest Neighbors (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_knn, auc_ci_cv_knn[0], auc_ci_cv_knn[1]))
axs[1].plot(fpr_cv_mlp, tpr_cv_mlp, color='purple', lw=2, label='Multilayer Perceptron (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_mlp, auc_ci_cv_mlp[0], auc_ci_cv_mlp[1]))
axs[1].plot(fpr_cv_svc, tpr_cv_svc, color='green', lw=2, label='Support Vector Machine (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_svc, auc_ci_cv_sv[0], auc_ci_cv_sv[1]))
axs[1].plot(fpr_cv_dt, tpr_cv_dt, color='gray', lw=2, label='Decision Tree (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_dt, auc_ci_cv_dt[0], auc_ci_cv_dt[1]))
axs[1].plot(fpr_cv_logreg, tpr_cv_logreg, color='red', lw=2, label='Logistic Regression (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_logreg, auc_ci_cv_lr[0], auc_ci_cv_lr[1]))

axs[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
axs[1].set_xlim([0.0, 1.0])
axs[1].set_ylim([0.0, 1.05])
axs[1].set_xlabel('False Positive Rate')
axs[1].set_ylabel('True Positive Rate')
axs[1].set_title('Receiver Operating Characteristic (ROC) Curve')
axs[1].legend(loc="lower right")
axs[0].set_title('Receiver Operating Characteristic - Test Data')
axs[1].set_title('Receiver Operating Characteristic - Cross-Validation (5-Folds)')
plt.savefig('ROCCURVE.jpg', dpi=600, bbox_inches='tight')
plt.show()

###FEATURE IMPORTANCE

X = df.drop('Risk', axis=1)
features = X.columns

colors = plt.cm.magma(np.linspace(0.2, 0.8, 5))  # Adicionei uma cor extra para evitar repetição

imp_features_gb = best_gb_model.feature_importances_
imp_features_lr = np.abs(best_logreg_model.coef_[0])
imp_features_dt = best_tree_clf.feature_importances_
imp_features_rf = best_rf_model.feature_importances_
imp_features_ada = best_ada_clf.feature_importances_  # Adicionando importância das features do AdaBoost

sorted_indices_gb = np.argsort(imp_features_gb)
sorted_indices_lr = np.argsort(imp_features_lr)
sorted_indices_dt = np.argsort(imp_features_dt)
sorted_indices_rf = np.argsort(imp_features_rf)
sorted_indices_ada = np.argsort(imp_features_ada)  # Ordenando para AdaBoost

fig, axs = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Feature Importance', fontsize=16)

# PLOT 1: Gradient Boosting
axs[0, 0].barh(features[sorted_indices_gb], imp_features_gb[sorted_indices_gb], color=colors[0])
axs[0, 0].set_title('Gradient Boosting')
axs[0, 0].set_xlabel('Importance')
axs[0, 0].set_ylabel('Feature')
axs[0, 0].grid(axis='x', linestyle='--', alpha=0.7)
axs[0, 0].set_facecolor('#f0f0f0')  # Cinza claro

# PLOT 2: Logistic Regression
axs[0, 1].barh(features[sorted_indices_lr], imp_features_lr[sorted_indices_lr], color=colors[1])
axs[0, 1].set_title('Logistic Regression')
axs[0, 1].set_xlabel('Importance')
axs[0, 1].set_ylabel('Feature')
axs[0, 1].grid(axis='x', linestyle='--', alpha=0.7)
axs[0, 1].set_facecolor('#f0f0f0')  # Cinza claro

# PLOT 3: Decision Tree
axs[0, 2].barh(features[sorted_indices_dt], imp_features_dt[sorted_indices_dt], color=colors[2])
axs[0, 2].set_title('Decision Tree')
axs[0, 2].set_xlabel('Importance')
axs[0, 2].set_ylabel('Feature')
axs[0, 2].grid(axis='x', linestyle='--', alpha=0.7)
axs[0, 2].set_facecolor('#f0f0f0')  # Cinza claro

# PLOT 4: Random Forest
axs[1, 0].barh(features[sorted_indices_rf], imp_features_rf[sorted_indices_rf], color=colors[3])
axs[1, 0].set_title('Random Forest')
axs[1, 0].set_xlabel('Importance')
axs[1, 0].set_ylabel('Feature')
axs[1, 0].grid(axis='x', linestyle='--', alpha=0.7)
axs[1, 0].set_facecolor('#f0f0f0')  # Cinza claro

# PLOT 5: Adaboosting
axs[1, 1].barh(features[sorted_indices_ada], imp_features_ada[sorted_indices_ada], color=colors[4])  # Usando a cor extra
axs[1, 1].set_title('AdaBoost')
axs[1, 1].set_xlabel('Importance')
axs[1, 1].set_ylabel('Feature')
axs[1, 1].grid(axis='x', linestyle='--', alpha=0.7)
axs[1, 1].set_facecolor('#f0f0f0')  # Cinza claro

fig.delaxes(axs[1, 2])

plt.tight_layout(rect=[0, 0, 1, 0.96], h_pad=1.5)

plt.savefig('FEATUREIMPORTANCE.jpg', dpi=300, bbox_inches='tight')
plt.show()

import numpy as np
from sklearn.metrics import roc_auc_score
from sklearn.utils import resample

# Definindo as probabilidades preditas para cada modelo
y_prob_test_gb = y_prob_test_gb
y_prob_test_lr = y_prob_test_logreg
y_prob_test_knn = y_prob_test_knn
y_prob_test_ada = y_prob_test_ada
y_prob_test_svc = y_prob_test_svc
y_prob_test_mlp = y_prob_test_mlp
y_prob_test_dt = y_prob_test_dt
y_prob_test_rf = y_prob_test_rf  # Adicionando o Random Forest

# Definindo as AUCs para cada modelo
roc_auc_gb = roc_auc_gb
roc_auc_knn = roc_auc_knn
roc_auc_ada = roc_auc_ada
roc_auc_svc = roc_auc_svc
roc_auc_mlp = roc_auc_mlp
roc_auc_lr = roc_auc_logreg
roc_auc_dt = roc_auc_dt
roc_auc_rf = roc_auc_rf  # Adicionando a AUC do Random Forest

# Função para comparar AUCs de dois modelos via bootstrapping
def compare_auc_bootstrap(y_true, y_prob1, y_prob2, n_iterations=1000, random_state=24):
    np.random.seed(random_state)
    auc_diffs = []
    for _ in range(n_iterations):
        # Resample with replacement
        indices = np.random.choice(len(y_true), len(y_true), replace=True)
        y_true_resampled = y_true[indices]
        y_prob1_resampled = y_prob1[indices]
        y_prob2_resampled = y_prob2[indices]

        # Calculate AUC for both models
        auc1 = roc_auc_score(y_true_resampled, y_prob1_resampled)
        auc2 = roc_auc_score(y_true_resampled, y_prob2_resampled)
        auc_diffs.append(auc1 - auc2)

    mean_diff = np.mean(auc_diffs)
    ci_diff = np.percentile(auc_diffs, [2.5, 97.5])

    return mean_diff, ci_diff

# Comparar AUCs de todos os pares de modelos, incluindo Random Forest
model_names = ["gb", "knn", "ada", "svc", "mlp", "lr", "dt", "rf"]
auc_scores = {
    "gb": roc_auc_gb,
    "knn": roc_auc_knn,
    "ada": roc_auc_ada,
    "svc": roc_auc_svc,
    "mlp": roc_auc_mlp,
    "lr": roc_auc_lr,
    "dt": roc_auc_dt,
    "rf": roc_auc_rf
}
y_probs = {
    "gb": y_prob_test_gb,
    "knn": y_prob_test_knn,
    "ada": y_prob_test_ada,
    "svc": y_prob_test_svc,
    "mlp": y_prob_test_mlp,
    "lr": y_prob_test_lr,
    "dt": y_prob_test_dt,
    "rf": y_prob_test_rf
}

# Comparar cada par de modelos
for i, model1 in enumerate(model_names):
    for model2 in model_names[i+1:]:
        mean_diff, ci_diff = compare_auc_bootstrap(np.array(y_test), np.array(y_probs[model1]), np.array(y_probs[model2]))
        print(f"Difference in AUC between {model1} and {model2}: {mean_diff:.3f} (95% CI: {ci_diff})")

import matplotlib.pyplot as plt
import numpy as np

# Definir os resultados dos testes
model_pairs = [("gb", "knn"), ("gb", "ada"), ("gb", "svc"),
               ("gb", "mlp"), ("gb", "lr"), ("gb", "dt"),
               ("gb", "rf"),  # Adicionando o Random Forest
               ("knn", "ada"), ("knn", "svc"), ("knn", "mlp"),
               ("knn", "lr"), ("knn", "dt"), ("knn", "rf"),  # Adicionando o Random Forest
               ("ada", "svc"), ("ada", "mlp"), ("ada", "lr"),
               ("ada", "dt"), ("ada", "rf"),  # Adicionando o Random Forest
               ("svc", "mlp"), ("svc", "lr"), ("svc", "dt"),
               ("svc", "rf"),  # Adicionando o Random Forest
               ("mlp", "lr"), ("mlp", "dt"), ("mlp", "rf"),  # Adicionando o Random Forest
               ("lr", "dt"), ("lr", "rf"),  # Adicionando o Random Forest
               ("dt", "rf")]  # Adicionando o Random Forest

mean_diffs = []
cis = []

# Calcular as diferenças de AUC e os CIs para cada par de modelos
for model1, model2 in model_pairs:
    mean_diff, ci_diff = compare_auc_bootstrap(np.array(y_test), np.array(y_probs[model1]), np.array(y_probs[model2]))
    mean_diffs.append(mean_diff)
    cis.append(ci_diff)

# Extrair os limites inferiores e superiores do CI para plotagem
lower_bounds = np.array([ci[0] for ci in cis])
upper_bounds = np.array([ci[1] for ci in cis])

# Labels para os modelos
model_labels = [f"{model_pair[0].upper()} vs {model_pair[1].upper()}" for model_pair in model_pairs]

# Cores e estilos para o gráfico
colors = ['b' if mean_diff >= 0 else 'r' for mean_diff in mean_diffs]  # Azul se positivo, vermelho se negativo
bar_colors = ['lightblue' if mean_diff >= 0 else 'lightcoral' for mean_diff in mean_diffs]  # Cores das barras
edge_colors = ['black' if mean_diff >= 0 else 'darkred' for mean_diff in mean_diffs]  # Cores das bordas das barras

# Criar o gráfico de barras com intervalo de confiança
plt.figure(figsize=(16, 10))
bars = plt.bar(model_labels, mean_diffs, yerr=[np.abs(lower_bounds - np.array(mean_diffs)), np.abs(upper_bounds - np.array(mean_diffs))], capsize=8, color=bar_colors, edgecolor=edge_colors)
plt.axhline(y=0, color='black', linestyle='--', linewidth=1.5)  # Linha horizontal em y=0 para referência
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel('Model Pairs', fontsize=14)
plt.ylabel('Difference in AUC', fontsize=14)
plt.title('Difference in AUC between Model Pairs with 95% CI', fontsize=16)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Adicionar legenda para as cores
legend_labels = ['Positive Difference', 'Negative Difference']
legend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in ['lightblue', 'lightcoral']]
plt.legend(legend_handles, legend_labels, loc='upper left', fontsize=12)

# Adicionar quadro de legenda para siglas dos modelos
legend_text = {
    "GB": "Gradient Boosting",
    "KNN": "K-Nearest Neighbors",
    "ADA": "AdaBoost",
    "SVC": "Support Vector Classifier",
    "MLP": "Multi-layer Perceptron",
    "LR": "Logistic Regression",
    "DT": "Decision Tree",
    "RF": "Random Forest"
}

plt.text(0.02, 0.02, "\n".join(f"{key}: {value}" for key, value in legend_text.items()), transform=plt.gca().transAxes, fontsize=12, verticalalignment='bottom', bbox=dict(facecolor='white', alpha=0.5))

plt.tight_layout()
plt.savefig('Meandifference.jpg', dpi=300, bbox_inches='tight')
plt.show()

"""# **Regressor Model building**"""

df2 = pd.read_excel(file_path)
colunas = ['Age ',  'Vita Classical (Baseline color)','Concentration',
       'pH', 'Risk_n','Number of sessions']

df2 = df2.loc[:, colunas]
df2 = df2.rename(columns={'Risk_n': 'Risk'})

coluna_grupo = df2.pop('Risk')
df2.insert(0, 'Risk', coluna_grupo)

X2 = df2.iloc[:,1:]
y2 = df2.iloc[:,0]

#Dataset split
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2,test_size = 0.2,random_state=RANDOM_STATE,stratify=y, shuffle=True)

##Data normalization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train2 = scaler.fit_transform(X_train2)
X_test2 = scaler.transform(X_test2)

print(f"Train data shape of X = {X_train2.shape} and Y = {y_train2.shape}")
print(f"Test data shape of X = {X_test2.shape} and Y = {y_test2.shape}")

"""**Gradient Boosting Regressor**"""

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo de Gradient Boosting
gb_model = GradientBoostingRegressor(random_state=RANDOM_STATE)

# Definir a grade de hiperparâmetros
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10]
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train2, y_train2)
best_params_gb = grid_search.best_params_

# Treinar o melhor modelo encontrado
best_gb_model_cl = GradientBoostingRegressor(**best_params_gb)

# Ajustar o modelo nos dados de treino
best_gb_model_cl.fit(X_train2, y_train2)
y_pred_test_gb2 = best_gb_model_cl.predict(X_test2)

# Calcular os erros no conjunto de teste
mse_gb = mean_squared_error(y_test2, y_pred_test_gb2)
mae_gb = mean_absolute_error(y_test2, y_pred_test_gb2)
rmse_gb = sqrt(mse_gb)
r2_gb = r2_score(y_test2, y_pred_test_gb2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_gb = bootstrap_metrics_test(best_gb_model_cl, X_test2, y_test2)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_gb = ci95(metrics_test_gb['mse'])
rmse_ci_test_gb = ci95(metrics_test_gb['rmse'])
mae_ci_test_gb = ci95(metrics_test_gb['mae'])
r2_ci_test_gb = ci95(metrics_test_gb['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_gb['mse'])} (95% CI: {mse_ci_test_gb})")
print(f"Test Data RMSE: {np.mean(metrics_test_gb['rmse'])} (95% CI: {rmse_ci_test_gb})")
print(f"Test Data MAE: {np.mean(metrics_test_gb['mae'])} (95% CI: {mae_ci_test_gb})")
print(f"Test Data R2: {np.mean(metrics_test_gb['r2'])} (95% CI: {r2_ci_test_gb})")

# Realizando validação cruzada
kf_gb = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_gb2 = cross_val_predict(best_gb_model_cl, X_train2, y_train2, cv=kf_gb)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_gb = bootstrap_cv_metrics_regression(best_gb_model_cl, X_train2, y_train2, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_gb = ci95(metrics_cv_gb['mse'])
rmse_ci_cv_gb = ci95(metrics_cv_gb['rmse'])
mae_ci_cv_gb = ci95(metrics_cv_gb['mae'])
r2_ci_cv_gb = ci95(metrics_cv_gb['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_gb['mse'])} (95% CI: {mse_ci_cv_gb})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_gb['rmse'])} (95% CI: {rmse_ci_cv_gb})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_gb['mae'])} (95% CI: {mae_ci_cv_gb})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_gb['r2'])} (95% CI: {r2_ci_cv_gb})")

##FEATURE IMPORTANCE GRADIENT BOOSTING CLASSIFIER
X = df.drop('Risk', axis=1)
features=[]
for columns in X.columns:
    features.append(columns)
imp_features = best_gb_model_cl.feature_importances_
importances = best_gb_model_cl.feature_importances_

feature_importance_gb = pd.DataFrame({'Feature': features, 'Importance': importances})

feature_importance_sorted = feature_importance_gb.sort_values('Importance', ascending=True)
colors = plt.cm.viridis(np.linspace(0.2, 1, len(feature_importance_sorted)))
plt.figure(figsize=(10, 6))
bars = plt.barh(feature_importance_sorted['Feature'], feature_importance_sorted['Importance'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')

for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')

plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()
plt.show()

"""**Linear Regression**"""

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo de regressão
regression_model = LinearRegression()

# Definir a grade de hiperparâmetros
param_grid = {
    'fit_intercept': [True, False],
    'copy_X': [True, False],
    'n_jobs': [-1, 1],
    'positive': [True, False]
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(regression_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train2, y_train2)
best_params_linear = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_linear_model_cl = LinearRegression(**best_params_linear)

# Ajustar o modelo nos dados de treino
best_linear_model_cl.fit(X_train2, y_train2)
y_pred_test_linear2 = best_linear_model_cl.predict(X_test2)

# Calcular os erros no conjunto de teste
mse_linear = mean_squared_error(y_test2, y_pred_test_linear2)
mae_linear = mean_absolute_error(y_test2, y_pred_test_linear2)
rmse_linear = sqrt(mse_linear)
r2_linear = r2_score(y_test2, y_pred_test_linear2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_linear = bootstrap_metrics_test(best_linear_model_cl, X_test2, y_test2)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_linear = ci95(metrics_test_linear['mse'])
rmse_ci_test_linear = ci95(metrics_test_linear['rmse'])
mae_ci_test_linear = ci95(metrics_test_linear['mae'])
r2_ci_test_linear = ci95(metrics_test_linear['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_linear['mse'])} (95% CI: {mse_ci_test_linear})")
print(f"Test Data RMSE: {np.mean(metrics_test_linear['rmse'])} (95% CI: {rmse_ci_test_linear})")
print(f"Test Data MAE: {np.mean(metrics_test_linear['mae'])} (95% CI: {mae_ci_test_linear})")
print(f"Test Data R2: {np.mean(metrics_test_linear['r2'])} (95% CI: {r2_ci_test_linear})")

# Realizando validação cruzada
kf_linear = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_linear2 = cross_val_predict(best_linear_model_cl, X_train2, y_train2, cv=kf_linear)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_linear = bootstrap_cv_metrics_regression(best_linear_model_cl, X_train2, y_train2, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_linear = ci95(metrics_cv_linear['mse'])
rmse_ci_cv_linear = ci95(metrics_cv_linear['rmse'])
mae_ci_cv_linear = ci95(metrics_cv_linear['mae'])
r2_ci_cv_linear = ci95(metrics_cv_linear['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_linear['mse'])} (95% CI: {mse_ci_cv_linear})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_linear['rmse'])} (95% CI: {rmse_ci_cv_linear})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_linear['mae'])} (95% CI: {mae_ci_cv_linear})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_linear['r2'])} (95% CI: {r2_ci_cv_linear})")

coefficients = best_linear_model_cl.coef_

feature_importance_lr = pd.DataFrame({'Feature': X.columns, 'Importance': np.abs(coefficients)})
feature_importance_lr = feature_importance_lr.sort_values('Importance', ascending=True)

colors = plt.cm.viridis(np.linspace(0.2, 1, len(feature_importance_lr)))
plt.figure(figsize=(10, 6))
bars = plt.barh(feature_importance_lr['Feature'], feature_importance_lr['Importance'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')

for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')

plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()
plt.show()

"""**SVM**"""

from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo SVM
svm_model = SVR()

# Definir a grade de hiperparâmetros
param_grid = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'degree': [2, 3, 4]
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train2, y_train2)
best_params_svm = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_svm_model_cl = SVR(**best_params_svm)

# Ajustar o modelo nos dados de treino
best_svm_model_cl.fit(X_train2, y_train2)
y_pred_test_svm2 = best_svm_model_cl.predict(X_test2)

# Calcular os erros no conjunto de teste
mse_svm = mean_squared_error(y_test2, y_pred_test_svm2)
mae_svm = mean_absolute_error(y_test2, y_pred_test_svm2)
rmse_svm = sqrt(mse_svm)
r2_svm = r2_score(y_test2, y_pred_test_svm2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_svm = bootstrap_metrics_test(best_svm_model_cl, X_test2, y_test2)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_svm = ci95(metrics_test_svm['mse'])
rmse_ci_test_svm = ci95(metrics_test_svm['rmse'])
mae_ci_test_svm = ci95(metrics_test_svm['mae'])
r2_ci_test_svm = ci95(metrics_test_svm['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_svm['mse'])} (95% CI: {mse_ci_test_svm})")
print(f"Test Data RMSE: {np.mean(metrics_test_svm['rmse'])} (95% CI: {rmse_ci_test_svm})")
print(f"Test Data MAE: {np.mean(metrics_test_svm['mae'])} (95% CI: {mae_ci_test_svm})")
print(f"Test Data R2: {np.mean(metrics_test_svm['r2'])} (95% CI: {r2_ci_test_svm})")

# Realizando validação cruzada
kf_svm = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_svm2 = cross_val_predict(best_svm_model_cl, X_train2, y_train2, cv=kf_svm)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_svm = bootstrap_cv_metrics_regression(best_svm_model_cl, X_train2, y_train2, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_svm = ci95(metrics_cv_svm['mse'])
rmse_ci_cv_svm = ci95(metrics_cv_svm['rmse'])
mae_ci_cv_svm = ci95(metrics_cv_svm['mae'])
r2_ci_cv_svm = ci95(metrics_cv_svm['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_svm['mse'])} (95% CI: {mse_ci_cv_svm})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_svm['rmse'])} (95% CI: {rmse_ci_cv_svm})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_svm['mae'])} (95% CI: {mae_ci_cv_svm})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_svm['r2'])} (95% CI: {r2_ci_cv_svm})")

"""**KNN**"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo KNN
knn_model = KNeighborsRegressor()

# Definir a grade de hiperparâmetros
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]  # p=1 para distância de Manhattan, p=2 para distância Euclidiana
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train2, y_train2)
best_params_knn = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_knn_model_cl = KNeighborsRegressor(**best_params_knn)

# Ajustar o modelo nos dados de treino
best_knn_model_cl.fit(X_train2, y_train2)
y_pred_test_knn2 = best_knn_model_cl.predict(X_test2)

# Calcular os erros no conjunto de teste
mse_knn = mean_squared_error(y_test2, y_pred_test_knn2)
mae_knn = mean_absolute_error(y_test2, y_pred_test_knn2)
rmse_knn = sqrt(mse_knn)
r2_knn = r2_score(y_test2, y_pred_test_knn2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_knn = bootstrap_metrics_test(best_knn_model_cl, X_test2, y_test2)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_knn = ci95(metrics_test_knn['mse'])
rmse_ci_test_knn = ci95(metrics_test_knn['rmse'])
mae_ci_test_knn = ci95(metrics_test_knn['mae'])
r2_ci_test_knn = ci95(metrics_test_knn['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_knn['mse'])} (95% CI: {mse_ci_test_knn})")
print(f"Test Data RMSE: {np.mean(metrics_test_knn['rmse'])} (95% CI: {rmse_ci_test_knn})")
print(f"Test Data MAE: {np.mean(metrics_test_knn['mae'])} (95% CI: {mae_ci_test_knn})")
print(f"Test Data R2: {np.mean(metrics_test_knn['r2'])} (95% CI: {r2_ci_test_knn})")

# Realizando validação cruzada
kf_knn = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_knn2 = cross_val_predict(best_knn_model_cl, X_train2, y_train2, cv=kf_knn)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_knn = bootstrap_cv_metrics_regression(best_knn_model_cl, X_train2, y_train2, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_knn = ci95(metrics_cv_knn['mse'])
rmse_ci_cv_knn = ci95(metrics_cv_knn['rmse'])
mae_ci_cv_knn = ci95(metrics_cv_knn['mae'])
r2_ci_cv_knn = ci95(metrics_cv_knn['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_knn['mse'])} (95% CI: {mse_ci_cv_knn})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_knn['rmse'])} (95% CI: {rmse_ci_cv_knn})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_knn['mae'])} (95% CI: {mae_ci_cv_knn})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_knn['r2'])} (95% CI: {r2_ci_cv_knn})")

"""**Random Forest**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo Random Forest
rf_model = RandomForestRegressor(random_state=RANDOM_STATE)

# Definir a grade de hiperparâmetros
param_grid = {
    'n_estimators': [50, 100, 200],  # Número de árvores na floresta
    'max_depth': [None, 10, 20],  # Profundidade máxima das árvores
    'min_samples_split': [2, 5, 10],  # Número mínimo de amostras necessárias para dividir um nó interno
    'min_samples_leaf': [1, 2, 4],  # Número mínimo de amostras necessárias para ser uma folha
    'max_features': ['auto', 'sqrt', 'log2']  # Número de features a serem consideradas para cada split
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train2, y_train2)
best_params_rf = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_rf_model_cl = RandomForestRegressor(**best_params_rf, random_state=42)

# Ajustar o modelo nos dados de treino
best_rf_model_cl.fit(X_train2, y_train2)
y_pred_test_rf2 = best_rf_model_cl.predict(X_test2)

# Calcular os erros no conjunto de teste
mse_rf = mean_squared_error(y_test2, y_pred_test_rf2)
mae_rf = mean_absolute_error(y_test2, y_pred_test_rf2)
rmse_rf = sqrt(mse_rf)
r2_rf = r2_score(y_test2, y_pred_test_rf2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_rf = bootstrap_metrics_test(best_rf_model_cl, X_test2, y_test2)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_rf = ci95(metrics_test_rf['mse'])
rmse_ci_test_rf = ci95(metrics_test_rf['rmse'])
mae_ci_test_rf = ci95(metrics_test_rf['mae'])
r2_ci_test_rf = ci95(metrics_test_rf['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_rf['mse'])} (95% CI: {mse_ci_test_rf})")
print(f"Test Data RMSE: {np.mean(metrics_test_rf['rmse'])} (95% CI: {rmse_ci_test_rf})")
print(f"Test Data MAE: {np.mean(metrics_test_rf['mae'])} (95% CI: {mae_ci_test_rf})")
print(f"Test Data R2: {np.mean(metrics_test_rf['r2'])} (95% CI: {r2_ci_test_rf})")

# Realizando validação cruzada
kf_rf = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_rf2 = cross_val_predict(best_rf_model_cl, X_train2, y_train2, cv=kf_rf)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_rf = bootstrap_cv_metrics_regression(best_rf_model_cl, X_train2, y_train2, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_rf = ci95(metrics_cv_rf['mse'])
rmse_ci_cv_rf = ci95(metrics_cv_rf['rmse'])
mae_ci_cv_rf = ci95(metrics_cv_rf['mae'])
r2_ci_cv_rf = ci95(metrics_cv_rf['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_rf['mse'])} (95% CI: {mse_ci_cv_rf})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_rf['rmse'])} (95% CI: {rmse_ci_cv_rf})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_rf['mae'])} (95% CI: {mae_ci_cv_rf})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_rf['r2'])} (95% CI: {r2_ci_cv_rf})")

X_3 = df2.drop('Risk', axis=1)
features_3 = []

for column in X_3.columns:
    features_3.append(column)

imp_features_dt3 = best_rf_model_cl.feature_importances_

df_imp_features_dt3 = pd.DataFrame({"features": features_3, "weights": imp_features_dt3})
df_imp_features_dt3_sorted = df_imp_features_dt3.sort_values(by='weights', ascending=True)

colors = plt.cm.viridis(np.linspace(0.2, 1, len(df_imp_features_dt3_sorted)))
plt.figure(figsize=(10, 6))
bars = plt.barh(df_imp_features_dt3_sorted['features'], df_imp_features_dt3_sorted['weights'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

plt.show()

"""**MLP Regressor**"""

from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo MLP Regressor
mlp_model = MLPRegressor(random_state=RANDOM_STATE)

# Definir a grade de hiperparâmetros
param_grid = {
    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  # Tamanhos das camadas ocultas
    'activation': ['relu', 'tanh', 'logistic'],  # Funções de ativação
    'alpha': [0.0001, 0.001, 0.01],  # Parâmetro de regularização L2
    'learning_rate': ['constant', 'invscaling', 'adaptive']  # Taxa de aprendizado
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(mlp_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train2, y_train2)
best_params_mlp = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_mlp_model_cl = MLPRegressor(**best_params_mlp, random_state=42)

# Ajustar o modelo nos dados de treino
best_mlp_model_cl.fit(X_train2, y_train2)
y_pred_test_mlp2 = best_mlp_model_cl.predict(X_test2)

# Calcular os erros no conjunto de teste
mse_mlp = mean_squared_error(y_test2, y_pred_test_mlp2)
mae_mlp = mean_absolute_error(y_test2, y_pred_test_mlp2)
rmse_mlp = sqrt(mse_mlp)
r2_mlp = r2_score(y_test2, y_pred_test_mlp2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_mlp = bootstrap_metrics_test(best_mlp_model_cl, X_test2, y_test2)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_mlp = ci95(metrics_test_mlp['mse'])
rmse_ci_test_mlp = ci95(metrics_test_mlp['rmse'])
mae_ci_test_mlp = ci95(metrics_test_mlp['mae'])
r2_ci_test_mlp = ci95(metrics_test_mlp['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_mlp['mse'])} (95% CI: {mse_ci_test_mlp})")
print(f"Test Data RMSE: {np.mean(metrics_test_mlp['rmse'])} (95% CI: {rmse_ci_test_mlp})")
print(f"Test Data MAE: {np.mean(metrics_test_mlp['mae'])} (95% CI: {mae_ci_test_mlp})")
print(f"Test Data R2: {np.mean(metrics_test_mlp['r2'])} (95% CI: {r2_ci_test_mlp})")

# Realizando validação cruzada
kf_mlp = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_mlp2 = cross_val_predict(best_mlp_model_cl, X_train2, y_train2, cv=kf_mlp)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_mlp = bootstrap_cv_metrics_regression(best_mlp_model_cl, X_train2, y_train2, cv=5)

mse_ci_cv_mlp = ci95(metrics_cv_mlp['mse'])
rmse_ci_cv_mlp = ci95(metrics_cv_mlp['rmse'])
mae_ci_cv_mlp = ci95(metrics_cv_mlp['mae'])
r2_ci_cv_mlp = ci95(metrics_cv_mlp['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_mlp['mse'])} (95% CI: {mse_ci_cv_mlp})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_mlp['rmse'])} (95% CI: {rmse_ci_cv_mlp})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_mlp['mae'])} (95% CI: {mae_ci_cv_mlp})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_mlp['r2'])} (95% CI: {r2_ci_cv_mlp})")

"""**Decision Tree**"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

tree_model = DecisionTreeRegressor(random_state=RANDOM_STATE)

param_grid = {
    'criterion': ['mse', 'friedman_mse', 'mae', 'poisson'],  # Critérios para a função de divisão
    'splitter': ['best', 'random'],  # Estratégia de divisão
    'max_depth': [None, 10, 20, 30],  # Profundidade máxima da árvore
    'min_samples_split': [2, 5, 10],  # Número mínimo de amostras necessárias para dividir um nó
    'min_samples_leaf': [1, 2, 4]  # Número mínimo de amostras necessárias para estar em um nó folha
}

grid_search = GridSearchCV(tree_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train2, y_train2)
best_params_tree = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)

best_tree_model_cl = DecisionTreeRegressor(**best_params_tree, random_state=42)

best_tree_model_cl.fit(X_train2, y_train2)
y_pred_test_tree2 = best_tree_model_cl.predict(X_test2)

mse_tree = mean_squared_error(y_test2, y_pred_test_tree2)
mae_tree = mean_absolute_error(y_test2, y_pred_test_tree2)
rmse_tree = sqrt(mse_tree)
r2_tree = r2_score(y_test2, y_pred_test_tree2)

def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

metrics_test_tree = bootstrap_metrics_test(best_tree_model_cl, X_test2, y_test2)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_tree = ci95(metrics_test_tree['mse'])
rmse_ci_test_tree = ci95(metrics_test_tree['rmse'])
mae_ci_test_tree = ci95(metrics_test_tree['mae'])
r2_ci_test_tree = ci95(metrics_test_tree['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_tree['mse'])} (95% CI: {mse_ci_test_tree})")
print(f"Test Data RMSE: {np.mean(metrics_test_tree['rmse'])} (95% CI: {rmse_ci_test_tree})")
print(f"Test Data MAE: {np.mean(metrics_test_tree['mae'])} (95% CI: {mae_ci_test_tree})")
print(f"Test Data R2: {np.mean(metrics_test_tree['r2'])} (95% CI: {r2_ci_test_tree})")

kf_tree = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_tree2 = cross_val_predict(best_tree_model_cl, X_train2, y_train2, cv=kf_tree)

def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

metrics_cv_tree = bootstrap_cv_metrics_regression(best_tree_model_cl, X_train2, y_train2, cv=5)

mse_ci_cv_tree = ci95(metrics_cv_tree['mse'])
rmse_ci_cv_tree = ci95(metrics_cv_tree['rmse'])
mae_ci_cv_tree = ci95(metrics_cv_tree['mae'])
r2_ci_cv_tree = ci95(metrics_cv_tree['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_tree['mse'])} (95% CI: {mse_ci_cv_tree})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_tree['rmse'])} (95% CI: {rmse_ci_cv_tree})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_tree['mae'])} (95% CI: {mae_ci_cv_tree})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_tree['r2'])} (95% CI: {r2_ci_cv_tree})")

#FEATURE IMPORTANCE DECISION TREE

X_2 = []
X_2
features_2 =[]
imp_features_dt2 = []
df_imp_features_dt2 = []
X_2 = df2.drop('Risk', axis=1)
features_2 = []

for column in X_2.columns:
    features_2.append(column)

imp_features_dt2 = best_tree_model_cl.feature_importances_
df_imp_features_dt2 = pd.DataFrame({"features": features_2, "weights": imp_features_dt2})
df_imp_features_dt2_sorted = df_imp_features_dt2.sort_values(by='weights', ascending=True)
plt.figure(figsize=(10, 6))
colors = plt.cm.cividis(np.linspace(0.2, 1, len(df_imp_features_dt2_sorted)))
bars = plt.barh(df_imp_features_dt2_sorted['features'], df_imp_features_dt2_sorted['weights'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

plt.show()

"""**Adaboosting Regressor**"""

from sklearn.ensemble import AdaBoostRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo AdaBoost
adaboost_model = AdaBoostRegressor(random_state=RANDOM_STATE)

param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1, 10],
    'loss': ['linear', 'square', 'exponential']
}

grid_search = GridSearchCV(adaboost_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train2, y_train2)
best_params_adaboost = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
best_adaboost_model_cl = AdaBoostRegressor(**best_params_adaboost, random_state=42)

best_adaboost_model_cl.fit(X_train2, y_train2)
y_pred_test_adaboost2 = best_adaboost_model_cl.predict(X_test2)

mse_adaboost = mean_squared_error(y_test2, y_pred_test_adaboost2)
mae_adaboost = mean_absolute_error(y_test2, y_pred_test_adaboost2)
rmse_adaboost = sqrt(mse_adaboost)
r2_adaboost = r2_score(y_test2, y_pred_test_adaboost2)

def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics


metrics_test_adaboost = bootstrap_metrics_test(best_adaboost_model_cl, X_test2, y_test2)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_adaboost = ci95(metrics_test_adaboost['mse'])
rmse_ci_test_adaboost = ci95(metrics_test_adaboost['rmse'])
mae_ci_test_adaboost = ci95(metrics_test_adaboost['mae'])
r2_ci_test_adaboost = ci95(metrics_test_adaboost['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_adaboost['mse'])} (95% CI: {mse_ci_test_adaboost})")
print(f"Test Data RMSE: {np.mean(metrics_test_adaboost['rmse'])} (95% CI: {rmse_ci_test_adaboost})")
print(f"Test Data MAE: {np.mean(metrics_test_adaboost['mae'])} (95% CI: {mae_ci_test_adaboost})")
print(f"Test Data R2: {np.mean(metrics_test_adaboost['r2'])} (95% CI: {r2_ci_test_adaboost})")

kf_adaboost = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_adaboost2 = cross_val_predict(best_adaboost_model_cl, X_train2, y_train2, cv=kf_adaboost)


def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics


metrics_cv_adaboost = bootstrap_cv_metrics_regression(best_adaboost_model_cl, X_train2, y_train2, cv=5)

mse_ci_cv_adaboost = ci95(metrics_cv_adaboost['mse'])
rmse_ci_cv_adaboost = ci95(metrics_cv_adaboost['rmse'])
mae_ci_cv_adaboost = ci95(metrics_cv_adaboost['mae'])
r2_ci_cv_adaboost = ci95(metrics_cv_adaboost['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_adaboost['mse'])} (95% CI: {mse_ci_cv_adaboost})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_adaboost['rmse'])} (95% CI: {rmse_ci_cv_adaboost})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_adaboost['mae'])} (95% CI: {mae_ci_cv_adaboost})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_adaboost['r2'])} (95% CI: {r2_ci_cv_adaboost})")

#FEATURE IMPORTANCE - ADABOOSTING

X_3 = df.drop('Risk', axis=1)
features_3 = X_3.columns.tolist()
imp_features_ada = best_adaboost_model_cl.feature_importances_

df_imp_features_ada = pd.DataFrame({"features": features_3, "weights": imp_features_ada})
df_imp_features_ada_sorted = df_imp_features_ada.sort_values(by='weights', ascending=True)

plt.figure(figsize=(10, 6))
colors = plt.cm.inferno(np.linspace(0.2, 1, len(df_imp_features_ada_sorted)))
bars = plt.barh(df_imp_features_ada_sorted['features'], df_imp_features_ada_sorted['weights'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')

for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')

plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

plt.show()

"""**Metrics for Regressor Models**"""

import numpy as np
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt


y_pred_gb = best_gb_model_cl.predict(X_test)
y_pred_lr = best_linear_model_cl.predict(X_test)
y_pred_svm = best_svm_model_cl.predict(X_test)
y_pred_knn = best_knn_model_cl.predict(X_test)
y_pred_rf = best_rf_model_cl.predict(X_test)
y_pred_mlp = best_mlp_model_cl.predict(X_test)
y_pred_dt = best_tree_model_cl.predict(X_test)
y_pred_ada = best_adaboost_model_cl.predict(X_test)


mae_gb = mean_absolute_error(y_test, y_pred_gb)
mae_lr = mean_absolute_error(y_test, y_pred_lr)
mae_svm = mean_absolute_error(y_test, y_pred_svm)
mae_knn = mean_absolute_error(y_test, y_pred_knn)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mae_mlp = mean_absolute_error(y_test, y_pred_mlp)
mae_dt = mean_absolute_error(y_test, y_pred_dt)
mae_ada = mean_absolute_error(y_test, y_pred_ada)

# MAE x bootstrapping
def compare_mae_bootstrap(y_true, y_pred1, y_pred2, n_iterations=1000, random_state=24):
    np.random.seed(random_state)
    mae_diffs = []
    for _ in range(n_iterations):
        # Resample with replacement
        indices = np.random.choice(len(y_true), len(y_true), replace=True)
        y_true_resampled = y_true[indices]
        y_pred1_resampled = y_pred1[indices]
        y_pred2_resampled = y_pred2[indices]

        # Calculate MAE for both models
        mae1 = mean_absolute_error(y_true_resampled, y_pred1_resampled)
        mae2 = mean_absolute_error(y_true_resampled, y_pred2_resampled)
        mae_diffs.append(mae1 - mae2)

    mean_diff = np.mean(mae_diffs)
    ci_diff = np.percentile(mae_diffs, [2.5, 97.5])

    return mean_diff, ci_diff

model_names = ["gb", "lr", "svm", "knn", "rf", "mlp", "dt", "ada"]
mae_scores = {
    "gb": mae_gb,
    "lr": mae_lr,
    "svm": mae_svm,
    "knn": mae_knn,
    "rf": mae_rf,
    "mlp": mae_mlp,
    "dt": mae_dt,
    "ada": mae_ada
}
y_preds = {
    "gb": y_pred_gb,
    "lr": y_pred_lr,
    "svm": y_pred_svm,
    "knn": y_pred_knn,
    "rf": y_pred_rf,
    "mlp": y_pred_mlp,
    "dt": y_pred_dt,
    "ada": y_pred_ada
}


for i, model1 in enumerate(model_names):
    for model2 in model_names[i+1:]:
        mean_diff, ci_diff = compare_mae_bootstrap(np.array(y_test), np.array(y_preds[model1]), np.array(y_preds[model2]))
        print(f"Difference in MAE between {model1} and {model2}: {mean_diff:.3f} (95% CI: {ci_diff})")

model_pairs = [("gb", "lr"), ("gb", "svm"), ("gb", "knn"),
               ("gb", "rf"), ("gb", "mlp"), ("gb", "dt"),
               ("gb", "ada"), ("lr", "svm"), ("lr", "knn"),
               ("lr", "rf"), ("lr", "mlp"), ("lr", "dt"),
               ("lr", "ada"), ("svm", "knn"), ("svm", "rf"),
               ("svm", "mlp"), ("svm", "dt"), ("svm", "ada"),
               ("knn", "rf"), ("knn", "mlp"), ("knn", "dt"),
               ("knn", "ada"), ("rf", "mlp"), ("rf", "dt"),
               ("rf", "ada"), ("mlp", "dt"), ("mlp", "ada"),
               ("dt", "ada")]

mean_diffs = []
cis = []


for model1, model2 in model_pairs:
    mean_diff, ci_diff = compare_mae_bootstrap(np.array(y_test), np.array(y_preds[model1]), np.array(y_preds[model2]))
    mean_diffs.append(mean_diff)
    cis.append(ci_diff)


lower_bounds = np.array([ci[0] for ci in cis])
upper_bounds = np.array([ci[1] for ci in cis])


model_labels = [f"{model_pair[0].upper()} vs {model_pair[1].upper()}" for model_pair in model_pairs]


colors = ['b' if mean_diff >= 0 else 'r' for mean_diff in mean_diffs]  # Azul se positivo, vermelho se negativo
bar_colors = ['lightblue' if mean_diff >= 0 else 'lightcoral' for mean_diff in mean_diffs]  # Cores das barras
edge_colors = ['black' if mean_diff >= 0 else 'darkred' for mean_diff in mean_diffs]  # Cores das bordas das barras

plt.figure(figsize=(16, 10))
bars = plt.bar(model_labels, mean_diffs, yerr=[np.abs(lower_bounds - np.array(mean_diffs)), np.abs(upper_bounds - np.array(mean_diffs))], capsize=8, color=bar_colors, edgecolor=edge_colors)
plt.axhline(y=0, color='black', linestyle='--', linewidth=1.5)  # Linha horizontal em y=0 para referência
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel('Model Pairs', fontsize=14)
plt.ylabel('Difference in MAE', fontsize=14)
plt.title('Difference in MAE between Model Pairs with 95% CI', fontsize=16)
plt.grid(axis='y', linestyle='--', alpha=0.7)


legend_labels = ['Positive Difference', 'Negative Difference']
legend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in ['lightblue', 'lightcoral']]
plt.legend(legend_handles, legend_labels, loc='upper left', fontsize=12)


legend_text = {
    "GB": "Gradient Boosting Regressor",
    "LR": "Linear Regression",
    "SVM": "Support Vector Machine Regressor",
    "KNN": "K-Nearest Neighbors Regressor",
    "RF": "Random Forest  Regressor",
    "MLP": "Multi-layer Perceptron Regressor",
    "DT": "Decision Tree Regressor",
    "ADA": "AdaBoost Regressor"
}

plt.text(0.02, 0.02, "\n".join(f"{key}: {value}" for key, value in legend_text.items()), transform=plt.gca().transAxes, fontsize=12, verticalalignment='bottom', bbox=dict(facecolor='white', alpha=0.5))

plt.tight_layout()
plt.savefig('Meandifference_reg.jpg', dpi=300, bbox_inches='tight')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

X = df.drop('Risk', axis=1)
features = X.columns

imp_features_gb = best_gb_model_cl.feature_importances_
imp_features_lr = np.abs(best_linear_model_cl.coef_.ravel())  # Garantir que é um array 1D
imp_features_dt = best_tree_model_cl.feature_importances_
imp_features_rf = best_rf_model_cl.feature_importances_
imp_features_ada = best_adaboost_model_cl.feature_importances_

sorted_indices_gb = np.argsort(imp_features_gb)
sorted_indices_lr = np.argsort(imp_features_lr)
sorted_indices_dt = np.argsort(imp_features_dt)
sorted_indices_rf = np.argsort(imp_features_rf)
sorted_indices_ada = np.argsort(imp_features_ada)

fig, axs = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Feature Importance', fontsize=16)

colors = plt.cm.viridis(np.linspace(0.2, 0.8, 5))


for ax in axs.flat:
    ax.set_facecolor('#f0f0f0')  # Fundo cinza claro
    ax.grid(True, which='both', axis='both', linestyle='--', alpha=0.7)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)

# PLOT 1: Gradient Boosting
axs[0, 0].barh(features[sorted_indices_gb], imp_features_gb[sorted_indices_gb], color=colors[0])
axs[0, 0].set_title('Gradient Boosting Regressor')
axs[0, 0].set_xlabel('Importance')
axs[0, 0].set_ylabel('Feature')

# PLOT 2: Logistic Regression
axs[0, 1].barh(features[sorted_indices_lr], imp_features_lr[sorted_indices_lr], color=colors[1])
axs[0, 1].set_title('Linear Regression')
axs[0, 1].set_xlabel('Importance')
axs[0, 1].set_ylabel('Feature')

# PLOT 3: Decision Tree
axs[0, 2].barh(features[sorted_indices_dt], imp_features_dt[sorted_indices_dt], color=colors[2])
axs[0, 2].set_title('Decision Tree Regressor')
axs[0, 2].set_xlabel('Importance')
axs[0, 2].set_ylabel('Feature')

# PLOT 4: Random Forest
axs[1, 0].barh(features[sorted_indices_rf], imp_features_rf[sorted_indices_rf], color=colors[3])
axs[1, 0].set_title('Random Forest Regressor')
axs[1, 0].set_xlabel('Importance')
axs[1, 0].set_ylabel('Feature')

# PLOT 5: AdaBoost
axs[1, 1].barh(features[sorted_indices_ada], imp_features_ada[sorted_indices_ada], color=colors[4])
axs[1, 1].set_title('AdaBoost Regressor')
axs[1, 1].set_xlabel('Importance')
axs[1, 1].set_ylabel('Feature')


fig.delaxes(axs[1, 2])

plt.tight_layout(rect=[0, 0, 1, 0.96], h_pad=1.5)
plt.savefig('FEATUREIMPORTANCE2.jpg', dpi=300, bbox_inches='tight')
plt.show()